#!/usr/bin/env python3
"""
Advanced Controversy Detection System
å¤šç»´åº¦æ™ºèƒ½äº‰è®®è¯é¢˜æ£€æµ‹ç³»ç»Ÿ - ä¸“ä¸ºAIå·¥å…·çƒ­ç‚¹äº‹ä»¶è®¾è®¡
"""

import json
import os
import re
import math
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass
from collections import Counter
import requests
from pathlib import Path

@dataclass
class ControversySignal:
    """äº‰è®®ä¿¡å·æ•°æ®ç»“æ„"""
    keyword: str
    source: str
    intensity: float  # äº‰è®®å¼ºåº¦ 0-100
    sentiment_score: float  # æƒ…æ„Ÿåˆ†æ•° -1åˆ°1
    time_decay_factor: float  # æ—¶é—´è¡°å‡å› å­
    context_relevance: float  # è¯­å¢ƒç›¸å…³æ€§
    social_amplification: float  # ç¤¾äº¤æ”¾å¤§æ•ˆåº”
    timestamp: str

@dataclass 
class ControversyAnalysis:
    """äº‰è®®åˆ†æç»“æœ"""
    topic: str
    overall_score: float  # ç»¼åˆäº‰è®®åˆ†æ•°
    confidence: float  # ç½®ä¿¡åº¦
    category: str  # äº‰è®®ç±»å‹
    signals: List[ControversySignal]
    risk_level: str  # é£é™©ç­‰çº§
    recommended_action: str  # æ¨èè¡ŒåŠ¨
    trend_prediction: str  # è¶‹åŠ¿é¢„æµ‹

class AdvancedControversyDetector:
    """
    é«˜çº§äº‰è®®æ£€æµ‹å™¨
    é›†æˆå¤šç»´åº¦åˆ†æã€æ—¶é—´æƒé‡ã€è¯­å¢ƒç†è§£å’Œç¤¾äº¤éªŒè¯
    """
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤šå±‚çº§äº‰è®®è¯åº“
        self.controversy_lexicon = self._build_advanced_lexicon()
        
        # æƒ…æ„Ÿå¼ºåº¦æ˜ å°„
        self.sentiment_weights = self._build_sentiment_weights()
        
        # æ—¶é—´è¡°å‡å‚æ•°
        self.time_decay_hours = {
            'critical': 6,    # ä¸¥é‡äº‰è®®6å°æ—¶å†…æƒé‡æœ€é«˜
            'major': 24,      # é‡å¤§äº‰è®®24å°æ—¶å†…ä¿æŒé«˜æƒé‡
            'minor': 72       # è½»å¾®äº‰è®®72å°æ—¶å†…æœ‰æ•ˆ
        }
        
        # ç¤¾äº¤éªŒè¯é˜ˆå€¼
        self.social_validation_threshold = 2  # è‡³å°‘2ä¸ªæ•°æ®æºç¡®è®¤
        
    def _build_advanced_lexicon(self) -> Dict[str, Dict[str, float]]:
        """æ„å»ºé«˜çº§äº‰è®®è¯åº“ï¼ŒåŒ…å«æƒé‡å’Œè¯­å¢ƒä¿¡æ¯"""
        return {
            'ai_specific_issues': {
                # AIç‰¹å®šé—®é¢˜è¯æ±‡
                'é™æ™º': 25.0, 'intelligence_drop': 25.0, 'æ™ºå•†ä¸‹é™': 25.0,
                'æ¨¡å‹é€€åŒ–': 20.0, 'model_degradation': 20.0, 'performance_decline': 20.0,
                'å“åº”å˜æ…¢': 15.0, 'slow_response': 15.0, 'å»¶è¿Ÿå¢åŠ ': 15.0,
                'å‡†ç¡®ç‡ä¸‹é™': 18.0, 'accuracy_drop': 18.0, 'quality_decline': 18.0,
                'æœåŠ¡ä¸­æ–­': 22.0, 'service_outage': 22.0, 'downtime': 22.0,
                'æ•°æ®æ³„éœ²': 30.0, 'data_breach': 30.0, 'privacy_leak': 30.0,
                'ç®—æ³•åè§': 20.0, 'algorithm_bias': 20.0, 'unfair_results': 20.0,
                'ç‰ˆæƒäº‰è®®': 25.0, 'copyright_dispute': 25.0, 'legal_issues': 25.0,
            },
            
            'intensity_amplifiers': {
                # å¼ºåº¦æ”¾å¤§è¯
                'ä¸¥é‡': 2.0, 'severe': 2.0, 'seriously': 2.0,
                'å¤§é‡': 1.8, 'massive': 1.8, 'widespread': 1.8,
                'é¢‘ç¹': 1.6, 'frequent': 1.6, 'repeatedly': 1.6,
                'çªç„¶': 1.5, 'suddenly': 1.5, 'abruptly': 1.5,
                'å®Œå…¨': 2.2, 'completely': 2.2, 'totally': 2.2,
            },
            
            'emotional_intensifiers': {
                # æƒ…æ„Ÿå¼ºåŒ–è¯
                'æ„¤æ€’': 20.0, 'angry': 20.0, 'furious': 25.0,
                'å¤±æœ›': 15.0, 'disappointed': 15.0, 'let_down': 15.0,
                'è´¨ç–‘': 12.0, 'question': 12.0, 'doubt': 12.0,
                'æŠ—è®®': 18.0, 'protest': 18.0, 'oppose': 18.0,
                'æŠµåˆ¶': 22.0, 'boycott': 22.0, 'resist': 18.0,
                'æŠ•è¯‰': 16.0, 'complain': 16.0, 'report_issue': 16.0,
            },
            
            'social_media_markers': {
                # ç½‘ç»œçƒ­è¯å’Œä¿šè¯­
                'ç¿»è½¦': 25.0, 'epic_fail': 25.0, 'disaster': 25.0,
                'æ‹‰èƒ¯': 20.0, 'sucks': 20.0, 'terrible': 20.0,
                'å‰²éŸ­èœ': 28.0, 'scam': 28.0, 'rip_off': 25.0,
                'æ™ºå•†ç¨': 22.0, 'waste_of_money': 18.0, 'overpriced': 15.0,
                'å‘': 18.0, 'trap': 18.0, 'misleading': 16.0,
                'é»‘å¿ƒ': 24.0, 'unethical': 20.0, 'dishonest': 18.0,
            },
            
            'urgency_indicators': {
                # ç´§æ€¥æ€§æŒ‡æ ‡
                'ç´§æ€¥': 2.5, 'urgent': 2.5, 'critical': 2.8,
                'ç«‹å³': 2.2, 'immediately': 2.2, 'asap': 2.0,
                'è­¦å‘Š': 2.3, 'warning': 2.3, 'alert': 2.1,
                'å±é™©': 2.6, 'dangerous': 2.6, 'risky': 2.2,
            },
            
            'temporal_markers': {
                # æ—¶é—´ç›¸å…³æ ‡è®°
                'æœ€è¿‘': 1.8, 'recently': 1.8, 'lately': 1.6,
                'ä»Šå¤©': 2.0, 'today': 2.0, 'right_now': 2.2,
                'åˆšåˆš': 2.5, 'just_now': 2.5, 'moments_ago': 2.3,
                'çªç ´æ€§': 1.5, 'breaking': 2.8, 'developing': 2.0,
            }
        }
    
    def _build_sentiment_weights(self) -> Dict[str, float]:
        """æ„å»ºæƒ…æ„Ÿæƒé‡æ˜ å°„"""
        return {
            'extremely_negative': -0.9,  # æåº¦è´Ÿé¢
            'very_negative': -0.7,       # éå¸¸è´Ÿé¢
            'negative': -0.5,            # è´Ÿé¢
            'slightly_negative': -0.3,   # è½»å¾®è´Ÿé¢
            'neutral': 0.0,              # ä¸­æ€§
            'slightly_positive': 0.3,    # è½»å¾®æ­£é¢
            'positive': 0.5,             # æ­£é¢
            'very_positive': 0.7,        # éå¸¸æ­£é¢
            'extremely_positive': 0.9    # æåº¦æ­£é¢
        }
    
    def analyze_controversy(self, text: str, source: str, timestamp: Optional[str] = None) -> ControversySignal:
        """åˆ†æå•ä¸ªæ–‡æœ¬çš„äº‰è®®æ€§"""
        if not timestamp:
            timestamp = datetime.now().isoformat()
        
        # 1. åŸºç¡€äº‰è®®åˆ†æ•°è®¡ç®—
        base_score = self._calculate_base_controversy_score(text)
        
        # 2. æƒ…æ„Ÿåˆ†æ
        sentiment_score = self._analyze_sentiment_intensity(text)
        
        # 3. æ—¶é—´è¡°å‡å› å­
        time_decay = self._calculate_time_decay(timestamp)
        
        # 4. è¯­å¢ƒç›¸å…³æ€§
        context_relevance = self._assess_context_relevance(text, source)
        
        # 5. ç¤¾äº¤æ”¾å¤§æ•ˆåº”
        social_amplification = self._estimate_social_amplification(text, source)
        
        # 6. ç»¼åˆå¼ºåº¦è®¡ç®—
        intensity = self._calculate_comprehensive_intensity(
            base_score, sentiment_score, time_decay, 
            context_relevance, social_amplification
        )
        
        # æå–ä¸»è¦å…³é”®è¯
        main_keyword = self._extract_primary_keyword(text)
        
        return ControversySignal(
            keyword=main_keyword,
            source=source,
            intensity=intensity,
            sentiment_score=sentiment_score,
            time_decay_factor=time_decay,
            context_relevance=context_relevance,
            social_amplification=social_amplification,
            timestamp=timestamp
        )
    
    def _calculate_base_controversy_score(self, text: str) -> float:
        """è®¡ç®—åŸºç¡€äº‰è®®åˆ†æ•°ï¼Œä½¿ç”¨åŠ æƒè¯æ±‡åŒ¹é…"""
        text_lower = text.lower()
        total_score = 0.0
        matched_terms = 0
        
        for category, terms in self.controversy_lexicon.items():
            for term, weight in terms.items():
                if category == 'intensity_amplifiers' or category == 'urgency_indicators' or category == 'temporal_markers':
                    continue  # è¿™äº›è¯æ±‡ç”¨äºä¿®é¥°ï¼Œä¸å•ç‹¬è®¡åˆ†
                    
                count = len(re.findall(r'\b' + re.escape(term.replace('_', r'\s*')) + r'\b', text_lower))
                if count > 0:
                    # åº”ç”¨å¼ºåº¦æ”¾å¤§å™¨æ•ˆæœ
                    amplifier = self._find_intensity_amplifier(text_lower, term)
                    score_contribution = weight * count * amplifier
                    total_score += score_contribution
                    matched_terms += count
        
        # æ ‡å‡†åŒ–åˆ†æ•° (0-100)
        if matched_terms == 0:
            return 0.0
        
        # ä½¿ç”¨å¯¹æ•°å‡½æ•°é˜²æ­¢åˆ†æ•°è¿‡é«˜
        normalized_score = min(100.0, 30 * math.log(1 + total_score / 10))
        return normalized_score
    
    def _find_intensity_amplifier(self, text: str, base_term: str) -> float:
        """å¯»æ‰¾å¼ºåº¦æ”¾å¤§å™¨è¯æ±‡"""
        amplifier = 1.0
        
        # åœ¨base_termå‰å20ä¸ªå­—ç¬¦å†…å¯»æ‰¾æ”¾å¤§å™¨
        term_pos = text.find(base_term.replace('_', ' '))
        if term_pos == -1:
            return amplifier
            
        context_start = max(0, term_pos - 50)
        context_end = min(len(text), term_pos + len(base_term) + 50)
        context = text[context_start:context_end]
        
        for category in ['intensity_amplifiers', 'urgency_indicators', 'temporal_markers']:
            for amp_term, amp_weight in self.controversy_lexicon[category].items():
                if amp_term.replace('_', ' ') in context:
                    amplifier = max(amplifier, amp_weight)
        
        return amplifier
    
    def _analyze_sentiment_intensity(self, text: str) -> float:
        """åˆ†ææƒ…æ„Ÿå¼ºåº¦ï¼Œè¿”å›-1åˆ°1çš„åˆ†æ•°"""
        negative_indicators = [
            'terrible', 'awful', 'horrible', 'disgusting', 'hate', 'angry',
            'ç³Ÿç³•', 'å¯æ€•', 'è®¨åŒ', 'æ„¤æ€’', 'å¤±æœ›', 'ä¸æ»¡'
        ]
        
        positive_indicators = [
            'great', 'amazing', 'excellent', 'wonderful', 'love', 'perfect',
            'å¾ˆå¥½', 'æ£’', 'ä¼˜ç§€', 'å®Œç¾', 'å–œæ¬¢', 'æ»¡æ„'
        ]
        
        text_lower = text.lower()
        negative_count = sum(1 for indicator in negative_indicators if indicator in text_lower)
        positive_count = sum(1 for indicator in positive_indicators if indicator in text_lower)
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        if negative_count + positive_count == 0:
            return 0.0
        
        sentiment_score = (positive_count - negative_count) / (positive_count + negative_count)
        return sentiment_score
    
    def _calculate_time_decay(self, timestamp: str) -> float:
        """è®¡ç®—æ—¶é—´è¡°å‡å› å­ï¼Œè¶Šè¿‘çš„æ—¶é—´æƒé‡è¶Šé«˜"""
        try:
            post_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            if post_time.tzinfo is None:
                post_time = post_time.replace(tzinfo=None)
            current_time = datetime.now()
            
            hours_elapsed = (current_time - post_time.replace(tzinfo=None)).total_seconds() / 3600
            
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼Œ24å°æ—¶å†…ä¿æŒé«˜æƒé‡
            if hours_elapsed <= 6:
                return 1.0  # 6å°æ—¶å†…å…¨æƒé‡
            elif hours_elapsed <= 24:
                return 0.8  # 24å°æ—¶å†…0.8æƒé‡
            elif hours_elapsed <= 72:
                return 0.6 * math.exp(-hours_elapsed / 72)  # 72å°æ—¶å†…æŒ‡æ•°è¡°å‡
            else:
                return 0.1  # 72å°æ—¶åä½æƒé‡
                
        except:
            return 0.5  # æ— æ³•è§£ææ—¶é—´æ—¶ç»™äºˆä¸­ç­‰æƒé‡
    
    def _assess_context_relevance(self, text: str, source: str) -> float:
        """è¯„ä¼°è¯­å¢ƒç›¸å…³æ€§ï¼Œåˆ¤æ–­äº‰è®®æ˜¯å¦ä¸AIå·¥å…·ç›¸å…³"""
        ai_context_keywords = [
            'ai', 'artificial intelligence', 'machine learning', 'chatgpt', 'gpt',
            'claude', 'gemini', 'copilot', 'midjourney', 'dall-e',
            'äººå·¥æ™ºèƒ½', 'AIå·¥å…·', 'æœºå™¨å­¦ä¹ ', 'ç®—æ³•', 'æ¨¡å‹'
        ]
        
        text_lower = text.lower()
        relevance_score = 0.0
        
        for keyword in ai_context_keywords:
            if keyword in text_lower:
                relevance_score += 0.2
        
        # æ ¹æ®æ•°æ®æºè°ƒæ•´ç›¸å…³æ€§
        source_weights = {
            'reddit_artificial': 1.2,
            'reddit_chatgpt': 1.3,
            'news_api': 1.0,
            'hackernews': 1.1,
            'rss_techcrunch': 1.1
        }
        
        source_weight = source_weights.get(source, 1.0)
        relevance_score *= source_weight
        
        return min(1.0, relevance_score)
    
    def _estimate_social_amplification(self, text: str, source: str) -> float:
        """ä¼°ç®—ç¤¾äº¤æ”¾å¤§æ•ˆåº”"""
        viral_indicators = [
            'viral', 'trending', 'breaking', 'everyone', 'massive', 
            'ç«äº†', 'çƒ­æœ', 'åˆ·å±', 'ç–¯ä¼ ', 'çˆ†æ–™'
        ]
        
        text_lower = text.lower()
        amplification = 1.0
        
        for indicator in viral_indicators:
            if indicator in text_lower:
                amplification += 0.3
        
        # ç¤¾äº¤åª’ä½“å¹³å°æœ‰æ›´é«˜çš„æ”¾å¤§æ•ˆåº”
        if 'reddit' in source:
            amplification *= 1.2
        elif 'hackernews' in source:
            amplification *= 1.1
        
        return min(2.0, amplification)
    
    def _calculate_comprehensive_intensity(self, base_score: float, sentiment: float, 
                                         time_decay: float, relevance: float, 
                                         amplification: float) -> float:
        """è®¡ç®—ç»¼åˆäº‰è®®å¼ºåº¦"""
        # è´Ÿé¢æƒ…æ„Ÿå¢å¼ºäº‰è®®ï¼Œæ­£é¢æƒ…æ„Ÿå‡å¼±äº‰è®®
        sentiment_modifier = 1.0 + (-sentiment * 0.5)  # è´Ÿé¢æƒ…æ„Ÿä¼šå¢åŠ äº‰è®®å¼ºåº¦
        
        # ç»¼åˆè®¡ç®—
        intensity = base_score * sentiment_modifier * time_decay * relevance * amplification
        
        # ç¡®ä¿åœ¨0-100èŒƒå›´å†…
        return min(100.0, max(0.0, intensity))
    
    def _extract_primary_keyword(self, text: str) -> str:
        """æå–ä¸»è¦å…³é”®è¯"""
        ai_tools = [
            'chatgpt', 'claude', 'gpt-4', 'gpt-5', 'gemini', 'perplexity',
            'character.ai', 'midjourney', 'dall-e', 'stable diffusion',
            'github copilot', 'anthropic', 'openai', 'google ai'
        ]
        
        text_lower = text.lower()
        for tool in ai_tools:
            if tool in text_lower:
                return tool.title()
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå·¥å…·ï¼Œæå–æœ€é‡è¦çš„äº‰è®®è¯
        for category, terms in self.controversy_lexicon.items():
            if category in ['ai_specific_issues', 'emotional_intensifiers']:
                for term, weight in sorted(terms.items(), key=lambda x: x[1], reverse=True):
                    if term.replace('_', ' ') in text_lower:
                        return term.replace('_', ' ').title()
        
        return "AIå·¥å…·äº‰è®®"
    
    def multi_source_controversy_analysis(self, signals: List[ControversySignal]) -> ControversyAnalysis:
        """å¤šæºäº‰è®®åˆ†æï¼Œæä¾›ç»¼åˆåˆ¤æ–­"""
        if not signals:
            return self._create_empty_analysis()
        
        # 1. è®¡ç®—ç»¼åˆåˆ†æ•°
        weighted_scores = []
        source_count = Counter(signal.source for signal in signals)
        
        for signal in signals:
            # å¤šæºéªŒè¯åŠ æƒ
            source_validation_weight = min(2.0, source_count[signal.source] * 0.5)
            weighted_score = signal.intensity * source_validation_weight
            weighted_scores.append(weighted_score)
        
        overall_score = sum(weighted_scores) / len(weighted_scores) if weighted_scores else 0
        
        # 2. è®¡ç®—ç½®ä¿¡åº¦
        unique_sources = len(set(signal.source for signal in signals))
        confidence = min(1.0, unique_sources / self.social_validation_threshold)
        
        # 3. ç¡®å®šäº‰è®®ç±»åˆ«å’Œé£é™©ç­‰çº§
        category, risk_level = self._categorize_controversy(overall_score, signals)
        
        # 4. ç”Ÿæˆæ¨èè¡ŒåŠ¨å’Œè¶‹åŠ¿é¢„æµ‹
        recommended_action = self._generate_recommendation(overall_score, risk_level, signals)
        trend_prediction = self._predict_trend(signals)
        
        # 5. é€‰æ‹©ä»£è¡¨æ€§ä¸»é¢˜
        topic = self._extract_representative_topic(signals)
        
        return ControversyAnalysis(
            topic=topic,
            overall_score=overall_score,
            confidence=confidence,
            category=category,
            signals=signals,
            risk_level=risk_level,
            recommended_action=recommended_action,
            trend_prediction=trend_prediction
        )
    
    def _categorize_controversy(self, score: float, signals: List[ControversySignal]) -> Tuple[str, str]:
        """åˆ†ç±»äº‰è®®ç±»å‹å’Œé£é™©ç­‰çº§"""
        # åˆ†æä¸»è¦äº‰è®®ç±»å‹
        categories = {
            'performance': ['é™æ™º', 'performance_decline', 'å“åº”å˜æ…¢', 'slow_response'],
            'reliability': ['æœåŠ¡ä¸­æ–­', 'service_outage', 'downtime', 'æ•…éšœ'],
            'privacy': ['æ•°æ®æ³„éœ²', 'data_breach', 'privacy_leak', 'éšç§'],
            'ethical': ['ç®—æ³•åè§', 'algorithm_bias', 'æ­§è§†', 'unfair'],
            'commercial': ['å‰²éŸ­èœ', 'scam', 'æ™ºå•†ç¨', 'overpriced'],
            'quality': ['è´¨é‡ä¸‹é™', 'quality_decline', 'å‡†ç¡®ç‡ä¸‹é™', 'accuracy_drop']
        }
        
        category_scores = {cat: 0 for cat in categories}
        
        for signal in signals:
            for cat, keywords in categories.items():
                if any(keyword in signal.keyword.lower() for keyword in keywords):
                    category_scores[cat] += signal.intensity
        
        primary_category = max(category_scores.items(), key=lambda x: x[1])[0] if any(category_scores.values()) else 'general'
        
        # ç¡®å®šé£é™©ç­‰çº§
        if score >= 70:
            risk_level = "ä¸¥é‡"  # Critical
        elif score >= 50:
            risk_level = "é‡å¤§"  # Major  
        elif score >= 30:
            risk_level = "ä¸­ç­‰"  # Moderate
        elif score >= 15:
            risk_level = "è½»å¾®"  # Minor
        else:
            risk_level = "æä½"  # Minimal
        
        return primary_category, risk_level
    
    def _generate_recommendation(self, score: float, risk_level: str, signals: List[ControversySignal]) -> str:
        """ç”Ÿæˆæ¨èè¡ŒåŠ¨"""
        if score >= 70:
            return "ç«‹å³ç”Ÿæˆçƒ­ç‚¹åˆ†ææ–‡ç« ï¼ŒæŠ“ä½æµé‡çº¢åˆ©ï¼Œæä¾›è§£å†³æ–¹æ¡ˆå’Œæ›¿ä»£å»ºè®®"
        elif score >= 50:
            return "ä¼˜å…ˆç”Ÿæˆæ·±åº¦åˆ†æå†…å®¹ï¼Œå¹³è¡¡æŠ¥é“äº‰è®®ç„¦ç‚¹å’Œå®¢è§‚è¯„ä¼°"
        elif score >= 30:
            return "é€‚åº¦å…³æ³¨ï¼Œå¯ç”Ÿæˆç›¸å…³å†…å®¹ä½†é¿å…è¿‡åº¦æ¸²æŸ“äº‰è®®"
        elif score >= 15:
            return "æŒç»­ç›‘æ§ï¼Œæ ¹æ®å‘å±•æƒ…å†µå†³å®šæ˜¯å¦ç”Ÿæˆå†…å®¹"
        else:
            return "æš‚ä¸ç”Ÿæˆäº‰è®®ç›¸å…³å†…å®¹ï¼Œå…³æ³¨å…¶ä»–è¯é¢˜"
    
    def _predict_trend(self, signals: List[ControversySignal]) -> str:
        """é¢„æµ‹äº‰è®®å‘å±•è¶‹åŠ¿"""
        if not signals:
            return "æ— æ³•é¢„æµ‹"
        
        # åˆ†ææ—¶é—´è¶‹åŠ¿
        recent_signals = [s for s in signals if s.time_decay_factor > 0.8]
        older_signals = [s for s in signals if s.time_decay_factor <= 0.8]
        
        if len(recent_signals) > len(older_signals) * 2:
            return "äº‰è®®çƒ­åº¦ä¸Šå‡ä¸­ï¼Œå»ºè®®å¯†åˆ‡å…³æ³¨"
        elif len(recent_signals) < len(older_signals) * 0.5:
            return "äº‰è®®çƒ­åº¦ä¸‹é™ä¸­ï¼Œå¯è€ƒè™‘ç†æ€§åˆ†ææ–‡ç« "
        else:
            return "äº‰è®®çƒ­åº¦ç¨³å®šï¼Œé€‚åˆæ·±åº¦åˆ†æ"
    
    def _extract_representative_topic(self, signals: List[ControversySignal]) -> str:
        """æå–ä»£è¡¨æ€§è¯é¢˜"""
        keyword_weights = Counter()
        
        for signal in signals:
            keyword_weights[signal.keyword] += signal.intensity
        
        if keyword_weights:
            return keyword_weights.most_common(1)[0][0]
        else:
            return "AIå·¥å…·äº‰è®®è¯é¢˜"
    
    def _create_empty_analysis(self) -> ControversyAnalysis:
        """åˆ›å»ºç©ºçš„åˆ†æç»“æœ"""
        return ControversyAnalysis(
            topic="æ— äº‰è®®æ£€æµ‹",
            overall_score=0.0,
            confidence=0.0,
            category="none",
            signals=[],
            risk_level="æ— ",
            recommended_action="æ­£å¸¸å†…å®¹ç”Ÿæˆ",
            trend_prediction="æ— è¶‹åŠ¿æ•°æ®"
        )
    
    def save_analysis_cache(self, analysis: ControversyAnalysis, filename: str = "controversy_analysis_cache.json"):
        """ä¿å­˜äº‰è®®åˆ†æç¼“å­˜"""
        cache_file = self.data_dir / filename
        
        cache_data = {
            "generated_at": datetime.now().isoformat(),
            "analysis": {
                "topic": analysis.topic,
                "overall_score": analysis.overall_score,
                "confidence": analysis.confidence,
                "category": analysis.category,
                "risk_level": analysis.risk_level,
                "recommended_action": analysis.recommended_action,
                "trend_prediction": analysis.trend_prediction,
                "signals_count": len(analysis.signals),
                "top_signals": [
                    {
                        "keyword": signal.keyword,
                        "source": signal.source,
                        "intensity": signal.intensity,
                        "sentiment_score": signal.sentiment_score
                    }
                    for signal in sorted(analysis.signals, key=lambda x: x.intensity, reverse=True)[:5]
                ]
            }
        }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ äº‰è®®åˆ†æç»“æœå·²ä¿å­˜åˆ° {cache_file}")


def main():
    """æµ‹è¯•é«˜çº§äº‰è®®æ£€æµ‹å™¨"""
    detector = AdvancedControversyDetector()
    
    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        ("ChatGPTæœ€è¿‘ä¸¥é‡é™æ™ºäº†ï¼Œå›ç­”è´¨é‡æ˜æ˜¾ä¸‹é™ï¼Œå¾ˆå¤šç”¨æˆ·éƒ½åœ¨æŠ±æ€¨", "reddit_chatgpt", datetime.now().isoformat()),
        ("Claude AI appears to have performance issues lately, users reporting slower responses", "news_api", datetime.now().isoformat()),
        ("OpenAIæœåŠ¡ä¸­æ–­å¯¼è‡´å¤§é‡ç”¨æˆ·æ— æ³•æ­£å¸¸ä½¿ç”¨", "rss_techcrunch", (datetime.now() - timedelta(hours=2)).isoformat()),
        ("Midjourneyæ¶¨ä»·è¢«æ‰¹è¯„æ˜¯å‰²éŸ­èœè¡Œä¸º", "hackernews", (datetime.now() - timedelta(hours=6)).isoformat()),
    ]
    
    print("ğŸ” é«˜çº§äº‰è®®æ£€æµ‹æµ‹è¯•å¼€å§‹...")
    
    signals = []
    for text, source, timestamp in test_cases:
        signal = detector.analyze_controversy(text, source, timestamp)
        signals.append(signal)
        
        print(f"\nğŸ“Š å•é¡¹åˆ†æç»“æœ:")
        print(f"  æ–‡æœ¬: {text[:50]}...")
        print(f"  å…³é”®è¯: {signal.keyword}")
        print(f"  äº‰è®®å¼ºåº¦: {signal.intensity:.1f}")
        print(f"  æƒ…æ„Ÿåˆ†æ•°: {signal.sentiment_score:.2f}")
        print(f"  æ—¶é—´è¡°å‡: {signal.time_decay_factor:.2f}")
        print(f"  è¯­å¢ƒç›¸å…³æ€§: {signal.context_relevance:.2f}")
    
    # å¤šæºç»¼åˆåˆ†æ
    print(f"\nğŸ§  å¤šæºç»¼åˆåˆ†æç»“æœ:")
    analysis = detector.multi_source_controversy_analysis(signals)
    
    print(f"  äº‰è®®è¯é¢˜: {analysis.topic}")
    print(f"  ç»¼åˆåˆ†æ•°: {analysis.overall_score:.1f}/100")
    print(f"  ç½®ä¿¡åº¦: {analysis.confidence:.1f}")
    print(f"  äº‰è®®ç±»å‹: {analysis.category}")
    print(f"  é£é™©ç­‰çº§: {analysis.risk_level}")
    print(f"  æ¨èè¡ŒåŠ¨: {analysis.recommended_action}")
    print(f"  è¶‹åŠ¿é¢„æµ‹: {analysis.trend_prediction}")
    
    # ä¿å­˜ç»“æœ
    detector.save_analysis_cache(analysis)
    
    print(f"\nâœ… é«˜çº§äº‰è®®æ£€æµ‹æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()