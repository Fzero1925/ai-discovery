#!/usr/bin/env python3
"""
Advanced Anti-AI Detection System
é«˜çº§åAIæ£€æµ‹ç³»ç»Ÿ - æ·±åº¦äººæ€§åŒ–å†™ä½œæ¨¡å¼
"""

import random
import re
import json
import math
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path

@dataclass
class HumanizationConfig:
    """äººæ€§åŒ–é…ç½®"""
    sentence_length_variance: float = 0.3  # å¥é•¿å˜å¼‚åº¦
    paragraph_structure_randomness: float = 0.25  # æ®µè½ç»“æ„éšæœºæ€§
    vocabulary_diversity_level: float = 0.8  # è¯æ±‡å¤šæ ·æ€§æ°´å¹³
    personal_voice_intensity: float = 0.6  # ä¸ªäººè¯­éŸ³å¼ºåº¦
    imperfection_level: float = 0.1  # ä¸å®Œç¾ç¨‹åº¦
    conversational_tone: float = 0.4  # å¯¹è¯è¯­è°ƒ

class AdvancedAntiAIDetection:
    """
    é«˜çº§åAIæ£€æµ‹ç³»ç»Ÿ
    å®ç°æ·±åº¦äººæ€§åŒ–å†™ä½œï¼Œè§„é¿AIæ£€æµ‹å·¥å…·
    """
    
    def __init__(self, config: Optional[HumanizationConfig] = None):
        self.config = config or HumanizationConfig()
        self.humanization_patterns = self._load_humanization_patterns()
        self.vocabulary_variations = self._load_vocabulary_variations()
        self.sentence_transformers = self._load_sentence_transformers()
        self.personal_voice_elements = self._load_personal_voice_elements()
        
    def _load_humanization_patterns(self) -> Dict[str, List]:
        """åŠ è½½äººæ€§åŒ–å†™ä½œæ¨¡å¼"""
        return {
            "sentence_starters": {
                "analytical": [
                    "ä»æˆ‘çš„è§‚å¯Ÿæ¥çœ‹ï¼Œ", "åŸºäºå®é™…ä½“éªŒï¼Œ", "æˆ‘å‘ç°", "å€¼å¾—æ³¨æ„çš„æ˜¯",
                    "æœ‰è¶£çš„æ˜¯", "ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯", "ä¸å¾—ä¸è¯´", "å¦ç‡åœ°è¯´",
                    "ä»å®ç”¨è§’åº¦è®²", "å®¢è§‚æ¥è¯´", "ç»¼åˆè€ƒè™‘å"
                ],
                "personal": [
                    "æˆ‘ä¸ªäººè®¤ä¸º", "åœ¨æˆ‘çœ‹æ¥", "æˆ‘çš„æ„Ÿå—æ˜¯", "å°±æˆ‘è€Œè¨€",
                    "ä»æˆ‘çš„ç»éªŒæ¥è¯´", "æˆ‘æ¯”è¾ƒå€¾å‘äº", "æˆ‘æ³¨æ„åˆ°", "æˆ‘å‘ç°",
                    "è®©æˆ‘å°è±¡æ·±åˆ»çš„æ˜¯", "æˆ‘å¿…é¡»æ‰¿è®¤"
                ],
                "casual": [
                    "è¯´å®è¯", "è€å®è¯´", "ä¸ç’ä½ è¯´", "ç®€å•æ¥è¯´", "æ¢å¥è¯è¯´",
                    "è¿™ä¹ˆè¯´å§", "æ‰“ä¸ªæ¯”æ–¹", "ä¸¾ä¸ªä¾‹å­", "æ€»çš„æ¥è¯´", "é¡ºä¾¿è¯´ä¸€ä¸‹"
                ]
            },
            
            "transition_phrases": {
                "logical": [
                    "å› æ­¤", "æ‰€ä»¥", "ç„¶è€Œ", "ä¸è¿‡", "å¦å¤–", "æ­¤å¤–", "åŒæ—¶",
                    "ç›¸æ¯”ä¹‹ä¸‹", "ä¸æ­¤åŒæ—¶", "è¯è¯´å›æ¥", "æ¢ä¸ªè§’åº¦çœ‹"
                ],
                "casual": [
                    "è¯åˆè¯´å›æ¥", "ä¸è¿‡è¯è¯´", "é¡ºä¾¿æä¸€ä¸‹", "è¯´åˆ°è¿™é‡Œ",
                    "å¦ä¸€æ–¹é¢", "ä»å¦ä¸€ä¸ªè§’åº¦", "è¿™æ ·çœ‹æ¥", "æ€»è€Œè¨€ä¹‹"
                ],
                "emphasis": [
                    "æ›´é‡è¦çš„æ˜¯", "å…³é”®åœ¨äº", "éœ€è¦å¼ºè°ƒçš„æ˜¯", "å€¼å¾—ä¸€æçš„æ˜¯",
                    "ç‰¹åˆ«æ˜¯", "å°¤å…¶æ˜¯", "æœ€å…³é”®çš„æ˜¯", "ä¸å®¹å¿½è§†çš„æ˜¯"
                ]
            },
            
            "sentence_endings": {
                "definitive": ["ã€‚", "ï¼", "..."],
                "questioning": ["ï¼Ÿ", "å§ï¼Ÿ", "å‘¢ï¼Ÿ"],
                "uncertain": ["ä¼¼ä¹æ˜¯è¿™æ ·", "å¤§æ¦‚æ˜¯", "åº”è¯¥è¯´", "å¯èƒ½"]
            },
            
            "paragraph_connectors": [
                "æ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹", "ç°åœ¨è¯´è¯´", "å†æ¥è°ˆè°ˆ", "å¦å¤–ä¸€ä¸ªæ–¹é¢",
                "æˆ‘ä»¬å†æ¥åˆ†æ", "è®©æˆ‘ä»¬æ·±å…¥äº†è§£", "å€¼å¾—æ¢è®¨çš„æ˜¯"
            ]
        }
    
    def _load_vocabulary_variations(self) -> Dict[str, List]:
        """åŠ è½½è¯æ±‡å˜æ¢åº“"""
        return {
            "professional_synonyms": {
                "åˆ†æ": ["è¯„ä¼°", "ç ”ç©¶", "è§£æ", "å‰–æ", "è€ƒå¯Ÿ", "å®¡è§†", "æ¢è®¨"],
                "ä½¿ç”¨": ["åº”ç”¨", "è¿ç”¨", "é‡‡ç”¨", "åˆ©ç”¨", "æ“ä½œ", "éƒ¨ç½²"],
                "åŠŸèƒ½": ["ç‰¹æ€§", "èƒ½åŠ›", "ç‰¹ç‚¹", "ä¼˜åŠ¿", "äº®ç‚¹", "å–ç‚¹"],
                "æ•ˆæœ": ["è¡¨ç°", "æˆæ•ˆ", "ç»“æœ", "æ•ˆæœ", "æˆæœ", "æ”¶æ•ˆ"],
                "é—®é¢˜": ["æŒ‘æˆ˜", "éš¾ç‚¹", "ç—‡ç»“", "ç“¶é¢ˆ", "å›°éš¾", "éšœç¢"],
                "ä¼˜åŠ¿": ["äº®ç‚¹", "å¼ºé¡¹", "é•¿å¤„", "ç‰¹è‰²", "ä¼˜ç‚¹", "ç‰¹é•¿"],
                "ç¼ºç‚¹": ["ä¸è¶³", "çŸ­æ¿", "å±€é™", "å¼±ç‚¹", "ç¼ºé™·", "é—®é¢˜"],
                "å»ºè®®": ["æ¨è", "æè®®", "å€¡è®®", "ä¸»å¼ ", "æ„è§", "çœ‹æ³•"],
                "é‡è¦": ["å…³é”®", "æ ¸å¿ƒ", "å…³é”®", "è¦ç´§", "é‡å¤§", "ç´§è¦"],
                "æå‡": ["æ”¹å–„", "å¢å¼º", "ä¼˜åŒ–", "æ”¹è¿›", "å¼ºåŒ–", "å‡çº§"]
            },
            
            "casual_expressions": {
                "å¾ˆå¥½": ["ä¸é”™", "æŒºæ£’", "ç›¸å½“å¥½", "å¾ˆèµ", "å€¼å¾—æ¨è", "è¡¨ç°å‡ºè‰²"],
                "ä¸€èˆ¬": ["ä¸­è§„ä¸­çŸ©", "è¿˜ç®—å¯ä»¥", "å·®å¼ºäººæ„", "é©¬é©¬è™è™", "ä¸­ç­‰æ°´å¹³"],
                "ä¸å¥½": ["æœ‰å¾…æ”¹è¿›", "ç•¥æ˜¾ä¸è¶³", "è¡¨ç°å¹³å¹³", "ä¸å¤ªç†æƒ³", "å­˜åœ¨é—®é¢˜"],
                "ç¡®å®": ["çš„ç¡®", "ç¡®å®", "çœŸçš„", "æ²¡é”™", "æ˜¯çš„", "å¯¹çš„"],
                "å¯èƒ½": ["ä¹Ÿè®¸", "å¤§æ¦‚", "æˆ–è®¸", "ä¼°è®¡", "å¯èƒ½", "åº”è¯¥"]
            },
            
            "technical_variations": {
                "ç®—æ³•": ["æŠ€æœ¯å®ç°", "æ ¸å¿ƒé€»è¾‘", "å¤„ç†æœºåˆ¶", "è®¡ç®—æ–¹æ³•"],
                "ç•Œé¢": ["ç”¨æˆ·ç•Œé¢", "æ“ä½œç•Œé¢", "äº¤äº’è®¾è®¡", "UIè®¾è®¡"],
                "æ•°æ®": ["ä¿¡æ¯", "å†…å®¹", "èµ„æ–™", "ç´ æ"],
                "ç³»ç»Ÿ": ["å¹³å°", "å·¥å…·", "è½¯ä»¶", "åº”ç”¨"],
                "æ€§èƒ½": ["è¡¨ç°", "æ•ˆç‡", "é€Ÿåº¦", "å“åº”"]
            }
        }
    
    def _load_sentence_transformers(self) -> Dict[str, callable]:
        """åŠ è½½å¥å­å˜æ¢å™¨"""
        return {
            "add_hesitation": self._add_hesitation_markers,
            "vary_sentence_length": self._vary_sentence_length,
            "add_personal_touch": self._add_personal_touch,
            "introduce_imperfection": self._introduce_subtle_imperfection,
            "casual_connector": self._add_casual_connectors,
            "questioning_tone": self._add_questioning_elements
        }
    
    def _load_personal_voice_elements(self) -> Dict[str, List]:
        """åŠ è½½ä¸ªäººè¯­éŸ³å…ƒç´ """
        return {
            "personal_experiences": [
                "åœ¨æˆ‘çš„å®é™…ä½¿ç”¨ä¸­", "é€šè¿‡æˆ‘çš„æµ‹è¯•", "æ ¹æ®æˆ‘çš„è§‚å¯Ÿ",
                "ä»æˆ‘çš„ä½“éªŒæ¥çœ‹", "åœ¨æˆ‘æ¥è§¦è¿‡çš„å·¥å…·ä¸­", "æˆ‘åœ¨é¡¹ç›®ä¸­å‘ç°",
                "é€šè¿‡ä¸åŒäº‹çš„è®¨è®º", "åŸºäºæˆ‘çš„ä¸“ä¸šåˆ¤æ–­"
            ],
            
            "emotional_responses": [
                "è®©æˆ‘å°è±¡æ·±åˆ»çš„æ˜¯", "ä»¤æˆ‘æ„å¤–çš„æ˜¯", "æˆ‘æ¯”è¾ƒå–œæ¬¢çš„æ˜¯",
                "æœ‰ç‚¹é—æ†¾çš„æ˜¯", "è®©æˆ‘æƒŠå–œçš„æ˜¯", "æˆ‘è§‰å¾—æ¯”è¾ƒé—æ†¾çš„æ˜¯",
                "è®©æˆ‘æ¯”è¾ƒæ»¡æ„çš„æ˜¯", "æˆ‘ä¸ªäººæ¯”è¾ƒåçˆ±"
            ],
            
            "uncertainty_markers": [
                "æˆ‘è§‰å¾—", "åœ¨æˆ‘çœ‹æ¥", "ä¸ªäººæ„Ÿè§‰", "æˆ‘çš„å°è±¡æ˜¯",
                "ä¼¼ä¹", "å¥½åƒ", "å¤§æ¦‚", "åº”è¯¥è¯´", "å¯èƒ½", "ä¹Ÿè®¸"
            ],
            
            "conversational_fillers": [
                "å—¯", "å‘¢", "å§", "å“¦", "é¢", "è¿™ä¸ª", "é‚£ä¸ª", "å°±æ˜¯è¯´",
                "æ€ä¹ˆè¯´å‘¢", "è¿™æ ·è¯´å§", "ç®€å•æ¥è¯´", "æ€»ä¹‹"
            ]
        }
    
    def humanize_content(self, content: str, context: Optional[Dict] = None) -> str:
        """å¯¹å†…å®¹è¿›è¡Œå…¨é¢äººæ€§åŒ–å¤„ç†"""
        content = self._preprocess_content(content)
        
        # åˆ†æ®µå¤„ç†
        paragraphs = content.split('\n\n')
        humanized_paragraphs = []
        
        for i, paragraph in enumerate(paragraphs):
            if paragraph.strip():
                humanized = self._humanize_paragraph(paragraph, i, context)
                humanized_paragraphs.append(humanized)
        
        # åå¤„ç†
        result = '\n\n'.join(humanized_paragraphs)
        result = self._post_process_content(result)
        
        return result
    
    def _preprocess_content(self, content: str) -> str:
        """é¢„å¤„ç†å†…å®¹"""
        # ç§»é™¤è¿‡äºè§„æ•´çš„æ ¼å¼
        content = re.sub(r'\n{3,}', '\n\n', content)
        # æ·»åŠ é€‚åº¦çš„æ ¼å¼å˜åŒ–
        return content
    
    def _humanize_paragraph(self, paragraph: str, paragraph_index: int, context: Optional[Dict]) -> str:
        """äººæ€§åŒ–å•ä¸ªæ®µè½"""
        sentences = self._split_sentences(paragraph)
        humanized_sentences = []
        
        for i, sentence in enumerate(sentences):
            if sentence.strip():
                humanized = self._humanize_sentence(sentence, i, paragraph_index, context)
                humanized_sentences.append(humanized)
        
        # æ®µè½çº§åˆ«çš„å¤„ç†
        result = ' '.join(humanized_sentences)
        result = self._add_paragraph_personality(result, paragraph_index)
        
        return result
    
    def _split_sentences(self, paragraph: str) -> List[str]:
        """æ™ºèƒ½åˆ†å¥"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å¥ï¼Œè€ƒè™‘ä¸­æ–‡æ ‡ç‚¹
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]\s*', paragraph)
        return [s.strip() for s in sentences if s.strip()]
    
    def _humanize_sentence(self, sentence: str, sentence_index: int, paragraph_index: int, context: Optional[Dict]) -> str:
        """äººæ€§åŒ–å•ä¸ªå¥å­"""
        # éšæœºåº”ç”¨å˜æ¢å™¨
        transformers = list(self.sentence_transformers.keys())
        num_transforms = random.randint(1, min(3, len(transformers)))
        selected_transforms = random.sample(transformers, num_transforms)
        
        result = sentence
        for transform_name in selected_transforms:
            transformer = self.sentence_transformers[transform_name]
            if random.random() < self._get_transform_probability(transform_name):
                result = transformer(result, sentence_index, paragraph_index)
        
        return result
    
    def _get_transform_probability(self, transform_name: str) -> float:
        """è·å–å˜æ¢æ¦‚ç‡"""
        probabilities = {
            "add_hesitation": self.config.conversational_tone * 0.3,
            "vary_sentence_length": 0.6,
            "add_personal_touch": self.config.personal_voice_intensity * 0.4,
            "introduce_imperfection": self.config.imperfection_level,
            "casual_connector": self.config.conversational_tone * 0.25,
            "questioning_tone": 0.15
        }
        return probabilities.get(transform_name, 0.2)
    
    def _add_hesitation_markers(self, sentence: str, sentence_index: int, paragraph_index: int) -> str:
        """æ·»åŠ çŠ¹è±«æ ‡è®°"""
        if random.random() < 0.3:
            markers = self.personal_voice_elements["uncertainty_markers"]
            marker = random.choice(markers)
            # åœ¨å¥å­å¼€å¤´æˆ–ä¸­é—´æ’å…¥
            if random.random() < 0.7:
                return f"{marker}ï¼Œ{sentence}"
            else:
                words = sentence.split()
                if len(words) > 3:
                    insert_pos = random.randint(1, len(words) - 2)
                    words.insert(insert_pos, f"{marker}ï¼Œ")
                    return ' '.join(words)
        return sentence
    
    def _vary_sentence_length(self, sentence: str, sentence_index: int, paragraph_index: int) -> str:
        """å˜åŒ–å¥å­é•¿åº¦"""
        words = sentence.split()
        if len(words) < 5:
            # æ‰©å±•çŸ­å¥
            if random.random() < 0.4:
                extensions = ["ç¡®å®å¦‚æ­¤", "è¿™ä¸€ç‚¹å¾ˆé‡è¦", "å€¼å¾—æ³¨æ„", "éœ€è¦è€ƒè™‘"]
                return f"{sentence}ï¼Œ{random.choice(extensions)}"
        elif len(words) > 15:
            # åˆ†è§£é•¿å¥
            if random.random() < 0.3:
                mid_point = len(words) // 2
                first_part = ' '.join(words[:mid_point])
                second_part = ' '.join(words[mid_point:])
                connectors = ["ã€‚åŒæ—¶", "ã€‚å¦å¤–", "ã€‚è€Œä¸”", "ã€‚ä¸è¿‡"]
                return f"{first_part}{random.choice(connectors)}ï¼Œ{second_part}"
        return sentence
    
    def _add_personal_touch(self, sentence: str, sentence_index: int, paragraph_index: int) -> str:
        """æ·»åŠ ä¸ªäººåŒ–å…ƒç´ """
        if random.random() < 0.3:
            personal_elements = self.personal_voice_elements["personal_experiences"]
            element = random.choice(personal_elements)
            return f"{element}ï¼Œ{sentence}"
        elif random.random() < 0.2:
            emotional_elements = self.personal_voice_elements["emotional_responses"]
            element = random.choice(emotional_elements)
            return f"{element}ï¼Œ{sentence}"
        return sentence
    
    def _introduce_subtle_imperfection(self, sentence: str, sentence_index: int, paragraph_index: int) -> str:
        """å¼•å…¥ç»†å¾®çš„ä¸å®Œç¾"""
        if random.random() < self.config.imperfection_level:
            # æ·»åŠ å£è¯­åŒ–è¡¨è¾¾
            fillers = self.personal_voice_elements["conversational_fillers"]
            if random.random() < 0.5:
                filler = random.choice(fillers)
                return f"{sentence}{filler}"
            # æˆ–è€…æ·»åŠ ä¸å¤ªæ­£å¼çš„è¿æ¥è¯
            elif random.random() < 0.3:
                casual_endings = ["å§", "å‘¢", "å•Š", "å˜›"]
                return f"{sentence}{random.choice(casual_endings)}"
        return sentence
    
    def _add_casual_connectors(self, sentence: str, sentence_index: int, paragraph_index: int) -> str:
        """æ·»åŠ éšæ„çš„è¿æ¥è¯"""
        if sentence_index > 0 and random.random() < 0.2:
            connectors = self.humanization_patterns["transition_phrases"]["casual"]
            connector = random.choice(connectors)
            return f"{connector}ï¼Œ{sentence}"
        return sentence
    
    def _add_questioning_elements(self, sentence: str, sentence_index: int, paragraph_index: int) -> str:
        """æ·»åŠ ç–‘é—®å…ƒç´ """
        if random.random() < 0.15:
            if sentence.endswith('ã€‚'):
                questioning_endings = ["å—ï¼Ÿ", "å‘¢ï¼Ÿ", "å¯¹å§ï¼Ÿ", "æ˜¯ä¸æ˜¯ï¼Ÿ"]
                return sentence[:-1] + random.choice(questioning_endings)
        return sentence
    
    def _add_paragraph_personality(self, paragraph: str, paragraph_index: int) -> str:
        """ä¸ºæ®µè½æ·»åŠ ä¸ªæ€§åŒ–å…ƒç´ """
        # æ®µè½å¼€å¤´çš„ä¸ªæ€§åŒ–
        if paragraph_index == 0:
            starters = self.humanization_patterns["sentence_starters"]["personal"]
            if random.random() < 0.3:
                starter = random.choice(starters)
                paragraph = f"{starter}ï¼Œ{paragraph}"
        
        # æ®µè½è¿æ¥çš„è‡ªç„¶åŒ–
        elif random.random() < 0.4:
            connectors = self.humanization_patterns["paragraph_connectors"]
            connector = random.choice(connectors)
            paragraph = f"{connector}ï¼Œ{paragraph}"
        
        return paragraph
    
    def _post_process_content(self, content: str) -> str:
        """åå¤„ç†å†…å®¹"""
        # è¯æ±‡å¤šæ ·åŒ–
        content = self._diversify_vocabulary(content)
        
        # è°ƒæ•´æ ‡ç‚¹ç¬¦å·çš„ä½¿ç”¨
        content = self._humanize_punctuation(content)
        
        # æœ€ç»ˆçš„è‡ªç„¶åŒ–å¤„ç†
        content = self._final_naturalization(content)
        
        return content
    
    def _diversify_vocabulary(self, content: str) -> str:
        """è¯æ±‡å¤šæ ·åŒ–"""
        for category, synonyms_dict in self.vocabulary_variations.items():
            for original, alternatives in synonyms_dict.items():
                if original in content:
                    # éšæœºæ›¿æ¢éƒ¨åˆ†è¯æ±‡
                    occurrences = content.count(original)
                    if occurrences > 1:
                        replace_count = max(1, int(occurrences * self.config.vocabulary_diversity_level))
                        for _ in range(replace_count):
                            if random.random() < 0.7:
                                alternative = random.choice(alternatives)
                                content = content.replace(original, alternative, 1)
        return content
    
    def _humanize_punctuation(self, content: str) -> str:
        """äººæ€§åŒ–æ ‡ç‚¹ç¬¦å·"""
        # å¶å°”ä½¿ç”¨çœç•¥å·
        content = re.sub(r'ã€‚(?=\s*[A-Z\u4e00-\u9fff])', lambda m: '...' if random.random() < 0.05 else 'ã€‚', content)
        
        # æ·»åŠ æ„Ÿå¹å·è¡¨è¾¾æƒ…æ„Ÿ
        content = re.sub(r'(\u4e00-\u9fff{3,}[å¾ˆéå¸¸ç‰¹åˆ«])([^ï¼ã€‚ï¼Ÿ]*?)ã€‚', 
                        lambda m: f"{m.group(1)}{m.group(2)}ï¼" if random.random() < 0.3 else m.group(0), content)
        
        return content
    
    def _final_naturalization(self, content: str) -> str:
        """æœ€ç»ˆè‡ªç„¶åŒ–å¤„ç†"""
        # è°ƒæ•´æ®µè½é—´è·çš„ä¸€è‡´æ€§
        content = re.sub(r'\n\n+', '\n\n', content)
        
        # ç§»é™¤è¿‡äºæœºæ¢°åŒ–çš„æ¨¡å¼
        content = re.sub(r'(åŒæ—¶|æ­¤å¤–|å¦å¤–|ç„¶è€Œ)ï¼Œ\s*\1', lambda m: m.group(1), content)
        
        return content.strip()
    
    def calculate_humanization_score(self, content: str) -> Dict[str, float]:
        """è®¡ç®—äººæ€§åŒ–åˆ†æ•°"""
        scores = {
            "sentence_variation": self._assess_sentence_variation(content),
            "vocabulary_diversity": self._assess_vocabulary_diversity(content),
            "personal_voice": self._assess_personal_voice(content),
            "natural_flow": self._assess_natural_flow(content),
            "imperfection_authenticity": self._assess_imperfection_level(content)
        }
        
        scores["overall_score"] = sum(scores.values()) / len(scores)
        return scores
    
    def _assess_sentence_variation(self, content: str) -> float:
        """è¯„ä¼°å¥å­å˜åŒ–åº¦"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        if len(sentences) < 2:
            return 0.5
        
        lengths = [len(s.split()) for s in sentences if s.strip()]
        if not lengths:
            return 0.5
        
        avg_length = sum(lengths) / len(lengths)
        variance = sum((l - avg_length) ** 2 for l in lengths) / len(lengths)
        
        # æ ‡å‡†åŒ–å˜å¼‚åº¦åˆ†æ•°
        return min(1.0, variance / (avg_length ** 2))
    
    def _assess_vocabulary_diversity(self, content: str) -> float:
        """è¯„ä¼°è¯æ±‡å¤šæ ·æ€§"""
        words = re.findall(r'\b\w+\b', content.lower())
        if not words:
            return 0.0
        
        unique_words = set(words)
        return len(unique_words) / len(words)
    
    def _assess_personal_voice(self, content: str) -> float:
        """è¯„ä¼°ä¸ªäººåŒ–ç¨‹åº¦"""
        personal_markers = sum(len(markers) for markers in self.personal_voice_elements.values())
        found_markers = 0
        
        for marker_category in self.personal_voice_elements.values():
            for marker in marker_category:
                if marker in content:
                    found_markers += content.count(marker)
        
        return min(1.0, found_markers / max(1, len(content.split()) // 20))
    
    def _assess_natural_flow(self, content: str) -> float:
        """è¯„ä¼°è‡ªç„¶æµç•…åº¦"""
        # ç®€åŒ–çš„æµç•…åº¦è¯„ä¼°
        mechanical_patterns = [
            r'(é¦–å…ˆ|å…¶æ¬¡|å†æ¬¡|æœ€å).*?ã€‚.*?(é¦–å…ˆ|å…¶æ¬¡|å†æ¬¡|æœ€å)',
            r'(å› æ­¤|æ‰€ä»¥|ç„¶è€Œ).*?ã€‚.*?(å› æ­¤|æ‰€ä»¥|ç„¶è€Œ)',
            r'^\d+\..*?^\d+\.',  # åˆ—è¡¨æ¨¡å¼
        ]
        
        penalty = 0
        for pattern in mechanical_patterns:
            matches = len(re.findall(pattern, content, re.MULTILINE))
            penalty += matches * 0.1
        
        return max(0.0, 1.0 - penalty)
    
    def _assess_imperfection_level(self, content: str) -> float:
        """è¯„ä¼°ä¸å®Œç¾ç¨‹åº¦ï¼ˆé€‚åº¦çš„ä¸å®Œç¾æé«˜çœŸå®æ€§ï¼‰"""
        imperfection_markers = ['å—¯', 'å‘¢', 'å§', 'å“¦', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æ€ä¹ˆè¯´', 'å¤§æ¦‚', 'å¯èƒ½', 'ä¹Ÿè®¸']
        found_imperfections = sum(content.count(marker) for marker in imperfection_markers)
        
        # é€‚åº¦çš„ä¸å®Œç¾æ˜¯å¥½çš„
        ideal_ratio = 0.02  # æ¯100ä¸ªå­—æœ‰2ä¸ªä¸å®Œç¾æ ‡è®°
        actual_ratio = found_imperfections / max(1, len(content))
        
        # è®¡ç®—ä¸ç†æƒ³æ¯”ä¾‹çš„æ¥è¿‘ç¨‹åº¦
        return 1.0 - abs(actual_ratio - ideal_ratio) / ideal_ratio


def main():
    """æµ‹è¯•é«˜çº§åAIæ£€æµ‹ç³»ç»Ÿ"""
    print("ğŸ­ æµ‹è¯•é«˜çº§åAIæ£€æµ‹ç³»ç»Ÿ...")
    
    # æµ‹è¯•æ–‡æœ¬
    test_content = """
è¿™æ˜¯ä¸€ä¸ªAIå·¥å…·çš„ä¸“ä¸šåˆ†æã€‚è¯¥å·¥å…·å…·æœ‰å¼ºå¤§çš„åŠŸèƒ½ï¼Œå¯ä»¥æå‡å·¥ä½œæ•ˆç‡ã€‚
ç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æ“ä½œæ¥ä½¿ç”¨è¿™äº›åŠŸèƒ½ã€‚è¯¥å·¥å…·çš„ç•Œé¢è®¾è®¡éå¸¸ä¸“ä¸šã€‚
æ€»ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå€¼å¾—æ¨èçš„AIå·¥å…·ã€‚å®ƒçš„æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œä»·æ ¼åˆç†ã€‚
"""
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    humanizer = AdvancedAntiAIDetection()
    
    print("ğŸ“ åŸå§‹å†…å®¹:")
    print(test_content)
    
    print("\nğŸ­ äººæ€§åŒ–åçš„å†…å®¹:")
    humanized_content = humanizer.humanize_content(test_content)
    print(humanized_content)
    
    print("\nğŸ“Š äººæ€§åŒ–è¯„åˆ†:")
    scores = humanizer.calculate_humanization_score(humanized_content)
    for metric, score in scores.items():
        print(f"  {metric}: {score:.3f}")
    
    print(f"\nâœ… é«˜çº§åAIæ£€æµ‹ç³»ç»Ÿæµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()