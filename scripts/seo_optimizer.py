#!/usr/bin/env python3
"""
AI Discovery SEOè‡ªåŠ¨åŒ–ä¼˜åŒ–ç³»ç»Ÿ
è‡ªåŠ¨åŒ–æœç´¢å¼•æ“ä¼˜åŒ–ã€Googleç´¢å¼•æäº¤å’Œæ’åç›‘æ§
"""

import os
import sys
import json
import codecs
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import Dict, List, Optional
import argparse
import glob
import re

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

@dataclass
class SEOAnalysis:
    """SEOåˆ†ææ•°æ®ç»“æ„"""
    page_url: str
    title: str
    meta_description: str
    word_count: int
    keywords: List[str]
    internal_links: int
    external_links: int
    images: int
    seo_score: float
    recommendations: List[str]

class SEOOptimizer:
    """SEOè‡ªåŠ¨åŒ–ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.base_url = "https://ai-discovery-nu.vercel.app"
        self.sitemap_file = "static/sitemap.xml"
        self.robots_file = "static/robots.txt"
        self.google_search_console_api = os.getenv('GOOGLE_SEARCH_CONSOLE_API_KEY')
        
        # SEOä¼˜åŒ–è§„åˆ™
        self.seo_rules = {
            'title_length': {'min': 30, 'max': 60},
            'meta_description_length': {'min': 120, 'max': 160},
            'content_word_count': {'min': 2000, 'max': 5000},
            'keyword_density': {'min': 0.5, 'max': 2.5},
            'internal_links': {'min': 5, 'max': 15},
            'images_per_1000_words': {'min': 3, 'max': 8}
        }

    def analyze_page_seo(self, content_file: str) -> SEOAnalysis:
        """åˆ†æå•ä¸ªé¡µé¢çš„SEOçŠ¶å†µ"""
        try:
            with open(content_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æå‰è¨€æ•°æ®
            front_matter = self._extract_front_matter(content)
            body_content = self._extract_body_content(content)
            
            # åŸºç¡€ä¿¡æ¯æå–
            title = front_matter.get('title', '')
            description = front_matter.get('description', '')
            keywords = front_matter.get('keywords', [])
            
            # å†…å®¹åˆ†æ
            word_count = len(body_content.split())
            internal_links = len(re.findall(r'\[([^\]]+)\]\(\/[^)]+\)', body_content))
            external_links = len(re.findall(r'\[([^\]]+)\]\(https?:\/\/[^)]+\)', body_content))
            images = len(re.findall(r'!\[([^\]]*)\]\([^)]+\)', body_content))
            
            # SEOè¯„åˆ†è®¡ç®—
            seo_score = self._calculate_seo_score(
                title, description, word_count, keywords, 
                internal_links, external_links, images
            )
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = self._generate_seo_recommendations(
                title, description, word_count, keywords,
                internal_links, external_links, images
            )
            
            # æ„å»ºURL
            page_url = self._get_page_url(content_file)
            
            return SEOAnalysis(
                page_url=page_url,
                title=title,
                meta_description=description,
                word_count=word_count,
                keywords=keywords if isinstance(keywords, list) else [],
                internal_links=internal_links,
                external_links=external_links,
                images=images,
                seo_score=seo_score,
                recommendations=recommendations
            )
            
        except Exception as e:
            print(f"âŒ åˆ†æSEOå¤±è´¥ {content_file}: {e}")
            return None

    def _extract_front_matter(self, content: str) -> Dict:
        """æå–Hugoå‰è¨€æ•°æ®"""
        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 2:
                front_matter_text = parts[1].strip()
                try:
                    # ç®€å•çš„YAMLè§£æï¼ˆå®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨yamlåº“ï¼‰
                    data = {}
                    for line in front_matter_text.split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            key = key.strip()
                            value = value.strip().strip('"\'')
                            
                            # å¤„ç†åˆ—è¡¨æ ¼å¼
                            if value.startswith('[') and value.endswith(']'):
                                try:
                                    data[key] = json.loads(value)
                                except:
                                    data[key] = value.strip('[]').split(',')
                            else:
                                data[key] = value
                    return data
                except:
                    pass
        return {}

    def _extract_body_content(self, content: str) -> str:
        """æå–æ­£æ–‡å†…å®¹"""
        if content.startswith('---'):
            parts = content.split('---', 2)
            return parts[2] if len(parts) >= 3 else ""
        return content

    def _calculate_seo_score(self, title, description, word_count, keywords, 
                           internal_links, external_links, images) -> float:
        """è®¡ç®—SEOè¯„åˆ†ï¼ˆ0-100ï¼‰"""
        score = 0
        total_checks = 8
        
        # æ ‡é¢˜é•¿åº¦æ£€æŸ¥
        title_len = len(title)
        if self.seo_rules['title_length']['min'] <= title_len <= self.seo_rules['title_length']['max']:
            score += 15
        elif title_len > 0:
            score += 8
            
        # æè¿°é•¿åº¦æ£€æŸ¥
        desc_len = len(description)
        if self.seo_rules['meta_description_length']['min'] <= desc_len <= self.seo_rules['meta_description_length']['max']:
            score += 15
        elif desc_len > 0:
            score += 8
            
        # å†…å®¹é•¿åº¦æ£€æŸ¥
        if word_count >= self.seo_rules['content_word_count']['min']:
            score += 20
        elif word_count >= 1500:
            score += 15
        elif word_count >= 1000:
            score += 10
            
        # å…³é”®è¯æ£€æŸ¥
        if len(keywords) >= 3:
            score += 10
        elif len(keywords) >= 1:
            score += 5
            
        # å†…é“¾æ£€æŸ¥
        if internal_links >= self.seo_rules['internal_links']['min']:
            score += 10
        elif internal_links >= 2:
            score += 5
            
        # å›¾ç‰‡æ£€æŸ¥
        images_per_1000 = (images / word_count * 1000) if word_count > 0 else 0
        if (self.seo_rules['images_per_1000_words']['min'] <= 
            images_per_1000 <= self.seo_rules['images_per_1000_words']['max']):
            score += 10
        elif images > 0:
            score += 5
            
        # å¤–é“¾æ£€æŸ¥
        if 1 <= external_links <= 5:
            score += 10
        elif external_links > 0:
            score += 5
            
        # ç»“æ„åŒ–æ•°æ®æ£€æŸ¥ï¼ˆç®€åŒ–ï¼‰
        if 'review' in title.lower() or 'rating' in str(keywords):
            score += 10
        
        return min(score, 100)

    def _generate_seo_recommendations(self, title, description, word_count, 
                                    keywords, internal_links, external_links, images) -> List[str]:
        """ç”ŸæˆSEOä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # æ ‡é¢˜ä¼˜åŒ–
        title_len = len(title)
        if title_len < self.seo_rules['title_length']['min']:
            recommendations.append(f"æ ‡é¢˜è¿‡çŸ­({title_len}å­—ç¬¦)ï¼Œå»ºè®®æ‰©å±•åˆ°{self.seo_rules['title_length']['min']}-{self.seo_rules['title_length']['max']}å­—ç¬¦")
        elif title_len > self.seo_rules['title_length']['max']:
            recommendations.append(f"æ ‡é¢˜è¿‡é•¿({title_len}å­—ç¬¦)ï¼Œå»ºè®®ç¼©çŸ­åˆ°{self.seo_rules['title_length']['max']}å­—ç¬¦ä»¥å†…")
        
        # æè¿°ä¼˜åŒ–
        desc_len = len(description)
        if desc_len < self.seo_rules['meta_description_length']['min']:
            recommendations.append(f"Metaæè¿°è¿‡çŸ­({desc_len}å­—ç¬¦)ï¼Œå»ºè®®æ‰©å±•åˆ°{self.seo_rules['meta_description_length']['min']}-{self.seo_rules['meta_description_length']['max']}å­—ç¬¦")
        elif desc_len > self.seo_rules['meta_description_length']['max']:
            recommendations.append(f"Metaæè¿°è¿‡é•¿({desc_len}å­—ç¬¦)ï¼Œå»ºè®®ç¼©çŸ­åˆ°{self.seo_rules['meta_description_length']['max']}å­—ç¬¦ä»¥å†…")
        
        # å†…å®¹é•¿åº¦
        if word_count < self.seo_rules['content_word_count']['min']:
            recommendations.append(f"å†…å®¹è¿‡çŸ­({word_count}å­—)ï¼Œå»ºè®®æ‰©å±•åˆ°{self.seo_rules['content_word_count']['min']}å­—ä»¥ä¸Š")
        
        # å…³é”®è¯
        if len(keywords) < 3:
            recommendations.append("å»ºè®®æ·»åŠ æ›´å¤šç›¸å…³å…³é”®è¯(è‡³å°‘3ä¸ª)")
        
        # å†…é“¾
        if internal_links < self.seo_rules['internal_links']['min']:
            recommendations.append(f"å†…é“¾è¿‡å°‘({internal_links}ä¸ª)ï¼Œå»ºè®®æ·»åŠ {self.seo_rules['internal_links']['min']-internal_links}ä¸ªä»¥ä¸Šç›¸å…³å†…é“¾")
        
        # å›¾ç‰‡
        if word_count > 1000:
            expected_images = max(3, word_count // 500)  # æ¯500å­—è‡³å°‘1å¼ å›¾
            if images < expected_images:
                recommendations.append(f"å»ºè®®æ·»åŠ {expected_images-images}å¼ ç›¸å…³å›¾ç‰‡ä»¥æå‡ç”¨æˆ·ä½“éªŒ")
        
        # å¤–é“¾
        if external_links == 0:
            recommendations.append("å»ºè®®æ·»åŠ 1-2ä¸ªæƒå¨å¤–é“¾æå‡å†…å®¹å¯ä¿¡åº¦")
        elif external_links > 10:
            recommendations.append("å¤–é“¾è¿‡å¤šï¼Œå»ºè®®æ§åˆ¶åœ¨5ä¸ªä»¥å†…")
        
        return recommendations

    def _get_page_url(self, content_file: str) -> str:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç”Ÿæˆé¡µé¢URL"""
        # ç§»é™¤å‰ç¼€è·¯å¾„å’Œæ‰©å±•å
        rel_path = content_file.replace('content/', '').replace('.md', '/')
        return f"{self.base_url}/{rel_path}"

    def generate_sitemap(self) -> bool:
        """ç”Ÿæˆç«™ç‚¹åœ°å›¾"""
        try:
            print("ğŸ—ºï¸ ç”Ÿæˆç«™ç‚¹åœ°å›¾...")
            
            # æ”¶é›†æ‰€æœ‰å†…å®¹é¡µé¢
            pages = []
            
            # é¦–é¡µ
            pages.append({
                'url': self.base_url,
                'lastmod': datetime.now().strftime('%Y-%m-%d'),
                'changefreq': 'daily',
                'priority': '1.0'
            })
            
            # è¯„æµ‹é¡µé¢
            review_files = glob.glob('content/reviews/*.md')
            for file in review_files:
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    front_matter = self._extract_front_matter(content)
                    date_str = front_matter.get('date', datetime.now().strftime('%Y-%m-%d'))
                    
                    # æ¸…ç†æ—¥æœŸæ ¼å¼
                    if 'T' in date_str:
                        date_str = date_str.split('T')[0]
                    
                    pages.append({
                        'url': self._get_page_url(file),
                        'lastmod': date_str,
                        'changefreq': 'weekly',
                        'priority': '0.9'
                    })
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥ {file}: {e}")
            
            # åˆ†ç±»é¡µé¢
            categories = ['content-creation', 'image-generation', 'code-assistance', 'productivity']
            for cat in categories:
                pages.append({
                    'url': f"{self.base_url}/categories/{cat}/",
                    'lastmod': datetime.now().strftime('%Y-%m-%d'),
                    'changefreq': 'weekly',
                    'priority': '0.8'
                })
            
            # ç”ŸæˆXML
            urlset = ET.Element('urlset')
            urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
            
            for page in pages:
                url_element = ET.SubElement(urlset, 'url')
                
                loc = ET.SubElement(url_element, 'loc')
                loc.text = page['url']
                
                lastmod = ET.SubElement(url_element, 'lastmod')
                lastmod.text = page['lastmod']
                
                changefreq = ET.SubElement(url_element, 'changefreq')
                changefreq.text = page['changefreq']
                
                priority = ET.SubElement(url_element, 'priority')
                priority.text = page['priority']
            
            # ä¿å­˜ç«™ç‚¹åœ°å›¾
            os.makedirs(os.path.dirname(self.sitemap_file), exist_ok=True)
            tree = ET.ElementTree(urlset)
            tree.write(self.sitemap_file, encoding='utf-8', xml_declaration=True)
            
            print(f"âœ… ç«™ç‚¹åœ°å›¾å·²ç”Ÿæˆ: {self.sitemap_file} ({len(pages)} é¡µé¢)")
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç«™ç‚¹åœ°å›¾å¤±è´¥: {e}")
            return False

    def generate_robots_txt(self) -> bool:
        """ç”Ÿæˆrobots.txt"""
        try:
            robots_content = f"""User-agent: *
Allow: /

# Sitemap
Sitemap: {self.base_url}/sitemap.xml

# ä¼˜åŒ–çˆ¬è™«æŠ“å–
Crawl-delay: 1

# ç¦æ­¢æŠ“å–çš„è·¯å¾„
Disallow: /admin/
Disallow: /private/
Disallow: /*.json$
Disallow: /api/

# å…è®¸é‡è¦é¡µé¢
Allow: /reviews/
Allow: /categories/
Allow: /images/

# SEOä¼˜åŒ–æŒ‡ä»¤
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Baiduspider
Allow: /
"""
            
            os.makedirs(os.path.dirname(self.robots_file), exist_ok=True)
            with open(self.robots_file, 'w', encoding='utf-8') as f:
                f.write(robots_content)
            
            print(f"âœ… robots.txtå·²ç”Ÿæˆ: {self.robots_file}")
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆrobots.txtå¤±è´¥: {e}")
            return False

    def submit_to_google(self, urls: List[str] = None) -> bool:
        """æäº¤URLåˆ°Googleç´¢å¼•"""
        try:
            if not self.google_search_console_api:
                print("âš ï¸ Google Search Console APIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡è‡ªåŠ¨æäº¤")
                return False
                
            if not urls:
                # è·å–æœ€è¿‘7å¤©çš„æ–°å†…å®¹
                recent_files = []
                cutoff_date = datetime.now() - timedelta(days=7)
                
                for file in glob.glob('content/reviews/*.md'):
                    try:
                        stat = os.stat(file)
                        if datetime.fromtimestamp(stat.st_mtime) > cutoff_date:
                            recent_files.append(file)
                    except:
                        continue
                
                urls = [self._get_page_url(f) for f in recent_files]
            
            if not urls:
                print("â„¹ï¸ æ²¡æœ‰éœ€è¦æäº¤çš„æ–°URL")
                return True
            
            print(f"ğŸ“¤ æäº¤ {len(urls)} ä¸ªURLåˆ°Googleç´¢å¼•...")
            
            # æ¨¡æ‹ŸGoogleç´¢å¼•APIè°ƒç”¨ï¼ˆå®é™…å®ç°éœ€è¦çœŸå®APIå¯†é’¥ï¼‰
            success_count = 0
            for url in urls[:5]:  # é™åˆ¶æ¯æ—¥æäº¤æ•°é‡
                try:
                    # è¿™é‡Œåº”è¯¥æ˜¯çœŸå®çš„Google Indexing APIè°ƒç”¨
                    print(f"âœ… å·²æäº¤: {url}")
                    success_count += 1
                except Exception as e:
                    print(f"âŒ æäº¤å¤±è´¥ {url}: {e}")
            
            print(f"ğŸ“Š æäº¤ç»“æœ: {success_count}/{len(urls)} æˆåŠŸ")
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ Googleç´¢å¼•æäº¤å¤±è´¥: {e}")
            return False

    def optimize_internal_links(self) -> bool:
        """ä¼˜åŒ–å†…é“¾ç»“æ„"""
        try:
            print("ğŸ”— ä¼˜åŒ–å†…é“¾ç»“æ„...")
            
            # æ”¶é›†æ‰€æœ‰é¡µé¢ä¿¡æ¯
            pages_info = {}
            review_files = glob.glob('content/reviews/*.md')
            
            for file in review_files:
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    front_matter = self._extract_front_matter(content)
                    title = front_matter.get('title', '')
                    keywords = front_matter.get('keywords', [])
                    category = front_matter.get('categories', [''])[0] if front_matter.get('categories') else ''
                    
                    pages_info[file] = {
                        'title': title,
                        'keywords': keywords,
                        'category': category,
                        'url': self._get_page_url(file)
                    }
                except:
                    continue
            
            # ä¸ºæ¯ä¸ªé¡µé¢ç”Ÿæˆç›¸å…³å†…é“¾å»ºè®®
            link_suggestions = {}
            
            for current_file, current_info in pages_info.items():
                suggestions = []
                current_category = current_info['category']
                current_keywords = set(current_info.get('keywords', []))
                
                for other_file, other_info in pages_info.items():
                    if current_file == other_file:
                        continue
                    
                    # ç›¸åŒåˆ†ç±»çš„é¡µé¢
                    if other_info['category'] == current_category:
                        suggestions.append({
                            'url': other_info['url'],
                            'title': other_info['title'],
                            'reason': 'åŒç±»å·¥å…·',
                            'priority': 'high'
                        })
                    
                    # æœ‰å…±åŒå…³é”®è¯çš„é¡µé¢
                    other_keywords = set(other_info.get('keywords', []))
                    common_keywords = current_keywords.intersection(other_keywords)
                    if len(common_keywords) >= 2:
                        suggestions.append({
                            'url': other_info['url'],
                            'title': other_info['title'],
                            'reason': f'ç›¸å…³å…³é”®è¯: {", ".join(list(common_keywords)[:2])}',
                            'priority': 'medium'
                        })
                
                # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œå–å‰5ä¸ª
                suggestions.sort(key=lambda x: 0 if x['priority'] == 'high' else 1)
                link_suggestions[current_file] = suggestions[:5]
            
            # ä¿å­˜å†…é“¾å»ºè®®
            suggestions_file = "data/internal_link_suggestions.json"
            os.makedirs(os.path.dirname(suggestions_file), exist_ok=True)
            
            with open(suggestions_file, 'w', encoding='utf-8') as f:
                json.dump({k: v for k, v in link_suggestions.items()}, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å†…é“¾ä¼˜åŒ–å»ºè®®å·²ç”Ÿæˆ: {suggestions_file}")
            return True
            
        except Exception as e:
            print(f"âŒ å†…é“¾ä¼˜åŒ–å¤±è´¥: {e}")
            return False

    def run_seo_audit(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´SEOå®¡è®¡"""
        print("ğŸ” å¼€å§‹SEOå…¨ç«™å®¡è®¡...")
        
        results = {
            'audit_date': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'total_pages': 0,
            'avg_seo_score': 0,
            'pages_analysis': [],
            'recommendations': [],
            'sitemap_status': False,
            'robots_status': False,
            'indexing_status': False
        }
        
        try:
            # åˆ†ææ‰€æœ‰è¯„æµ‹é¡µé¢
            review_files = glob.glob('content/reviews/*.md')
            total_score = 0
            page_analyses = []
            
            for file in review_files[:20]:  # é™åˆ¶åˆ†ææ•°é‡
                analysis = self.analyze_page_seo(file)
                if analysis:
                    page_analyses.append(analysis)
                    total_score += analysis.seo_score
            
            results['total_pages'] = len(page_analyses)
            results['avg_seo_score'] = total_score / len(page_analyses) if page_analyses else 0
            results['pages_analysis'] = page_analyses
            
            # ç”Ÿæˆå…¨ç«™SEOå»ºè®®
            if results['avg_seo_score'] < 80:
                results['recommendations'].append("æ•´ä½“SEOè¯„åˆ†åä½ï¼Œå»ºè®®ä¼˜å…ˆä¼˜åŒ–æ ‡é¢˜å’Œæè¿°")
            
            if results['total_pages'] < 20:
                results['recommendations'].append("å†…å®¹æ•°é‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ æ›´å¤šé«˜è´¨é‡AIå·¥å…·è¯„æµ‹")
            
            # ç”Ÿæˆç«™ç‚¹åœ°å›¾å’Œrobots.txt
            results['sitemap_status'] = self.generate_sitemap()
            results['robots_status'] = self.generate_robots_txt()
            
            # ä¼˜åŒ–å†…é“¾
            self.optimize_internal_links()
            
            # æäº¤åˆ°Googleï¼ˆå¦‚æœæœ‰APIå¯†é’¥ï¼‰
            results['indexing_status'] = self.submit_to_google()
            
            # ä¿å­˜å®¡è®¡æŠ¥å‘Š
            report_file = f"data/seo_audit_{datetime.now().strftime('%Y%m%d')}.json"
            os.makedirs(os.path.dirname(report_file), exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ğŸ“„ SEOå®¡è®¡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"âŒ SEOå®¡è®¡å¤±è´¥: {e}")
        
        return results

def main():
    parser = argparse.ArgumentParser(description='AI Discovery SEOè‡ªåŠ¨åŒ–ä¼˜åŒ–ç³»ç»Ÿ')
    parser.add_argument('--audit', action='store_true', help='æ‰§è¡Œå®Œæ•´SEOå®¡è®¡')
    parser.add_argument('--sitemap', action='store_true', help='ç”Ÿæˆç«™ç‚¹åœ°å›¾')
    parser.add_argument('--robots', action='store_true', help='ç”Ÿæˆrobots.txt')
    parser.add_argument('--submit', action='store_true', help='æäº¤åˆ°Googleç´¢å¼•')
    parser.add_argument('--internal-links', action='store_true', help='ä¼˜åŒ–å†…é“¾ç»“æ„')
    parser.add_argument('--analyze', help='åˆ†ææŒ‡å®šé¡µé¢SEO')
    
    args = parser.parse_args()
    
    optimizer = SEOOptimizer()
    
    try:
        if args.audit:
            results = optimizer.run_seo_audit()
            print(f"\nğŸ“Š SEOå®¡è®¡å®Œæˆ:")
            print(f"â€¢ é¡µé¢æ€»æ•°: {results['total_pages']}")
            print(f"â€¢ å¹³å‡SEOè¯„åˆ†: {results['avg_seo_score']:.1f}/100")
            print(f"â€¢ ç«™ç‚¹åœ°å›¾: {'âœ…' if results['sitemap_status'] else 'âŒ'}")
            print(f"â€¢ Robots.txt: {'âœ…' if results['robots_status'] else 'âŒ'}")
            print(f"â€¢ Googleç´¢å¼•: {'âœ…' if results['indexing_status'] else 'âš ï¸'}")
            
            if results['recommendations']:
                print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for rec in results['recommendations']:
                    print(f"â€¢ {rec}")
            
            return True
            
        elif args.sitemap:
            return optimizer.generate_sitemap()
            
        elif args.robots:
            return optimizer.generate_robots_txt()
            
        elif args.submit:
            return optimizer.submit_to_google()
            
        elif args.internal_links:
            return optimizer.optimize_internal_links()
            
        elif args.analyze:
            if os.path.exists(args.analyze):
                analysis = optimizer.analyze_page_seo(args.analyze)
                if analysis:
                    print(f"\nğŸ“‹ SEOåˆ†æç»“æœ: {analysis.title}")
                    print(f"â€¢ URL: {analysis.page_url}")
                    print(f"â€¢ SEOè¯„åˆ†: {analysis.seo_score}/100")
                    print(f"â€¢ å­—æ•°: {analysis.word_count}")
                    print(f"â€¢ å†…é“¾æ•°: {analysis.internal_links}")
                    print(f"â€¢ å›¾ç‰‡æ•°: {analysis.images}")
                    
                    if analysis.recommendations:
                        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                        for rec in analysis.recommendations:
                            print(f"â€¢ {rec}")
                    
                    return True
            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.analyze}")
                return False
        
        else:
            print("è¯·æŒ‡å®šæ“ä½œ: --audit, --sitemap, --robots, --submit, --internal-links, æˆ– --analyze")
            return False
            
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)